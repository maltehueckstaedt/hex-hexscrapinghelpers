{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyM6YnB4D+y1tw+SDLUePemA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maltehueckstaedt/hex-hexscrapinghelpers/blob/scrape_his_python_colab/scraping_his_wuppertal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping Universität Rostock\n",
        "\n",
        "## Pakete Laden\n",
        "\n",
        "In einem ersten Schritt laden wir die nötigen Pakete und starten mit `driver = gs.Chrome()` den Chromedriver headless."
      ],
      "metadata": {
        "id": "2o5LHDMK6mc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-colab-selenium\n",
        "import google_colab_selenium as gs\n",
        "import time\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "driver = gs.Chrome()"
      ],
      "metadata": {
        "id": "2ickjCvCvPfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vorbereitung des Scraping der Basisdaten\n",
        "\n",
        "`select_semester_and_set_courses`  "
      ],
      "metadata": {
        "id": "y6QebIkL61hD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "\n",
        "def select_semester_and_set_courses(driver, base_url, num_sem_selector, num_courses,\n",
        "                                     css_sem_dropdown, css_search_field, num_courses_selector):\n",
        "    try:\n",
        "        driver.get(base_url)\n",
        "        time.sleep(1)\n",
        "\n",
        "        # Dropdown klicken\n",
        "        dropdown = driver.find_element(By.CSS_SELECTOR, css_sem_dropdown)\n",
        "        dropdown.click()\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        # Dynamischen Selector bauen\n",
        "        base_selector = css_sem_dropdown.rsplit(\"_\", 1)[0]\n",
        "        full_selector = f\"{base_selector}_{num_sem_selector}\"\n",
        "        semester = driver.find_element(By.CSS_SELECTOR, full_selector)\n",
        "        semester.click()\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        # Suche auslösen\n",
        "        search_field = driver.find_element(By.CSS_SELECTOR, css_search_field)\n",
        "        search_field.click()\n",
        "        search_field.send_keys(Keys.ENTER)\n",
        "        time.sleep(1)\n",
        "\n",
        "        # Seite nach oben scrollen\n",
        "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
        "        time.sleep(0.2)\n",
        "\n",
        "        # Eingabefeld leeren + neue Kursanzahl setzen\n",
        "        input_field = driver.find_element(By.CSS_SELECTOR, num_courses_selector)\n",
        "        input_field.send_keys(Keys.CONTROL + \"a\")\n",
        "        input_field.send_keys(Keys.BACKSPACE)\n",
        "        input_field.send_keys(str(num_courses))\n",
        "        input_field.send_keys(Keys.ENTER)\n",
        "        time.sleep(1)\n",
        "\n",
        "        print(\"✔️ Fertig.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Fehler: {e}\")\n",
        "        return\n"
      ],
      "metadata": {
        "id": "zmSXNh8iF9yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://www.xn--studilwe-s4a.uni-wuppertal.de/qisserver/pages/cm/exa/coursemanagement/basicCourseData.xhtml?_flowId=searchCourseNonStaff-flow&_flowExecutionKey=e3s1\"\n",
        "sem_dropdown = \"#genericSearchMask\\\\:search_e4ff321960e251186ac57567bec9f4ce\\\\:cm_exa_eventprocess_basic_data\\\\:fieldset\\\\:inputField_3_abb156a1126282e4cf40d48283b4e76d\\\\:idabb156a1126282e4cf40d48283b4e76d\\\\:termSelect_label\"\n",
        "num_sem_selector = 2\n",
        "num_courses_selector = \"#genSearchRes\\\\:id3f3bd34c5d6b1c79\\\\:id3f3bd34c5d6b1c79Navi2NumRowsInput\"\n",
        "num_courses = \"300\"\n",
        "search_field = \"#genericSearchMask\\\\:search_e4ff321960e251186ac57567bec9f4ce\\\\:cm_exa_eventprocess_basic_data\\\\:fieldset\\\\:inputField_0_1ad08e26bde39c9e4f1833e56dcce9b5\\\\:id1ad08e26bde39c9e4f1833e56dcce9b5\"\n",
        "\n",
        "select_semester_and_set_courses(driver, base_url, num_sem_selector, num_courses,\n",
        "                                sem_dropdown, search_field, num_courses_selector)"
      ],
      "metadata": {
        "id": "LnreUjDCHJu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "import datetime\n",
        "\n",
        "def screenshot_fullpage(driver, filename=None):\n",
        "    # Default-Dateiname mit Zeitstempel\n",
        "    if filename is None:\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "        filename = f\"fullpage_screenshot_{timestamp}.png\"\n",
        "\n",
        "    print(\"→ Berechne Seitenhöhe und -breite...\")\n",
        "    total_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    total_width = driver.execute_script(\"return document.body.scrollWidth\")\n",
        "\n",
        "    print(f\"→ Setze Fenstergröße: {total_width}x{total_height}\")\n",
        "    driver.set_window_size(total_width, total_height)\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    print(f\"→ Speichere Screenshot unter: {filename}\")\n",
        "    driver.save_screenshot(filename)\n",
        "\n",
        "    print(\"→ Zeige Screenshot an:\")\n",
        "    return Image(filename)"
      ],
      "metadata": {
        "id": "EqpAhX2iKsyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "screenshot_fullpage(driver)\n"
      ],
      "metadata": {
        "id": "cUvWuvI3LJ68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install html5lib\n",
        "\n",
        "#!pip install lxml html5lib"
      ],
      "metadata": {
        "id": "EJzt6iOvfenj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from tqdm.notebook import tqdm  # für Google Colab\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "def click_next_page(driver, css_next_page=\"#genSearchRes\\\\:id3f3bd34c5d6b1c79\\\\:id3f3bd34c5d6b1c79Navi2next\", max_attempts=10):\n",
        "    for attempt in range(1, max_attempts + 1):\n",
        "\n",
        "\n",
        "        # Seite nach unten scrollen\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(1)\n",
        "\n",
        "        try:\n",
        "            weiter_button = driver.find_element(By.CSS_SELECTOR, css_next_page)\n",
        "            weiter_button.click()\n",
        "            time.sleep(3)\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "        # Prüfe, ob wieder auf der Übersichtsseite (nach erfolgreichem Klick)\n",
        "        try:\n",
        "            driver.find_element(By.CSS_SELECTOR, \"#genSearchRes\\\\:genericSearchResult > div.text_white_searchresult > span\")\n",
        "            break\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "\n",
        "def clean_prefixes(df):\n",
        "    for col in df.columns:\n",
        "        prefix = str(col).strip()\n",
        "        df[col] = df[col].astype(str).str.replace(f\"^{re.escape(prefix)}\", \"\", regex=True).str.strip()\n",
        "        df[col] = df[col].replace(\"nan\", pd.NA)\n",
        "    return df\n",
        "\n",
        "\n",
        "def scrape_all_pages(driver, css_max_selector, max_pages=None):\n",
        "    # Seitenanzahl ermitteln\n",
        "    selector = driver.find_element(By.CSS_SELECTOR, css_max_selector)\n",
        "    selector_text = selector.text.strip()\n",
        "    match = re.search(r\"\\d+$\", selector_text)\n",
        "    if not match:\n",
        "        raise ValueError(\"Konnte Seitenzahl nicht extrahieren.\")\n",
        "    select_end = int(match.group())\n",
        "\n",
        "    if max_pages is not None:\n",
        "        select_end = min(select_end, max_pages)\n",
        "\n",
        "    all_tables = []\n",
        "\n",
        "    for i in tqdm(range(1, select_end + 1), desc=\"Scraping Seiten\", unit=\"Seite\"):\n",
        "        print(f\"\\n→ Starte Scraping der Base-Informationen für Seite {i}\")\n",
        "\n",
        "        table_element = driver.find_element(By.CSS_SELECTOR, \"#genSearchRes\\\\:id3f3bd34c5d6b1c79\\\\:id3f3bd34c5d6b1c79Table\")\n",
        "        table_html = table_element.get_attribute(\"outerHTML\")\n",
        "\n",
        "        soup = BeautifulSoup(table_html, \"html.parser\")\n",
        "        table = pd.read_html(StringIO(str(soup)), header=0, flavor=\"bs4\")[0]\n",
        "        table = clean_prefixes(table)\n",
        "        all_tables.append(table)\n",
        "\n",
        "        click_next_page(driver)\n",
        "        time.sleep(3)\n",
        "\n",
        "    return pd.concat(all_tables, ignore_index=True)"
      ],
      "metadata": {
        "id": "0qIXSp_eLcOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "css_max_selector = \"#genSearchRes\\\\:id3f3bd34c5d6b1c79\\\\:id3f3bd34c5d6b1c79Navi2_div > div > span.dataScrollerPageText\"\n",
        "base_info = scrape_all_pages(driver, css_max_selector, max_pages=None)"
      ],
      "metadata": {
        "id": "pAiYCjRcLkgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_info"
      ],
      "metadata": {
        "id": "mG_9eLBZhjAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: ich würde gern base_info speichern\n",
        "\n",
        "base_info.to_csv('base_info.csv', index=False)"
      ],
      "metadata": {
        "id": "50vcSTYhifEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l49d_03XrYdV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}