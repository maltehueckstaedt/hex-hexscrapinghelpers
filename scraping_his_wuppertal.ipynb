{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPp8CRxLIApMnPN76wVLtJN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maltehueckstaedt/hex-hexscrapinghelpers/blob/scrape_his_python_colab/scraping_his_wuppertal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping Universität Rostock\n",
        "\n",
        "## Pakete Laden\n",
        "\n",
        "In einem ersten Schritt laden wir die nötigen Pakete und starten mit `driver = gs.Chrome()` den Chromedriver headless."
      ],
      "metadata": {
        "id": "2o5LHDMK6mc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-colab-selenium\n",
        "import google_colab_selenium as gs\n",
        "import time\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "driver = gs.Chrome()"
      ],
      "metadata": {
        "id": "2ickjCvCvPfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vorbereitung des Scraping der Basisdaten\n",
        "\n",
        "`select_semester_and_set_courses`  "
      ],
      "metadata": {
        "id": "y6QebIkL61hD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "\n",
        "def select_semester_and_set_courses(driver, base_url, num_sem_selector, num_courses,\n",
        "                                     css_sem_dropdown, css_search_field, num_courses_selector):\n",
        "    try:\n",
        "        driver.get(base_url)\n",
        "        time.sleep(1)\n",
        "\n",
        "        # Dropdown klicken\n",
        "        dropdown = driver.find_element(By.CSS_SELECTOR, css_sem_dropdown)\n",
        "        dropdown.click()\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        # Dynamischen Selector bauen\n",
        "        base_selector = css_sem_dropdown.rsplit(\"_\", 1)[0]\n",
        "        full_selector = f\"{base_selector}_{num_sem_selector}\"\n",
        "        semester = driver.find_element(By.CSS_SELECTOR, full_selector)\n",
        "        semester.click()\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        # Suche auslösen\n",
        "        search_field = driver.find_element(By.CSS_SELECTOR, css_search_field)\n",
        "        search_field.click()\n",
        "        search_field.send_keys(Keys.ENTER)\n",
        "        time.sleep(1)\n",
        "\n",
        "        # Seite nach oben scrollen\n",
        "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
        "        time.sleep(0.2)\n",
        "\n",
        "        # Eingabefeld leeren + neue Kursanzahl setzen\n",
        "        input_field = driver.find_element(By.CSS_SELECTOR, num_courses_selector)\n",
        "        input_field.send_keys(Keys.CONTROL + \"a\")\n",
        "        input_field.send_keys(Keys.BACKSPACE)\n",
        "        input_field.send_keys(str(num_courses))\n",
        "        input_field.send_keys(Keys.ENTER)\n",
        "        time.sleep(1)\n",
        "\n",
        "        print(\"✔️ Fertig.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Fehler: {e}\")\n",
        "        return\n"
      ],
      "metadata": {
        "id": "zmSXNh8iF9yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://www.xn--studilwe-s4a.uni-wuppertal.de/qisserver/pages/cm/exa/coursemanagement/basicCourseData.xhtml?_flowId=searchCourseNonStaff-flow&_flowExecutionKey=e3s1\"\n",
        "sem_dropdown = \"#genericSearchMask\\\\:search_e4ff321960e251186ac57567bec9f4ce\\\\:cm_exa_eventprocess_basic_data\\\\:fieldset\\\\:inputField_3_abb156a1126282e4cf40d48283b4e76d\\\\:idabb156a1126282e4cf40d48283b4e76d\\\\:termSelect_label\"\n",
        "num_sem_selector = 2\n",
        "num_courses_selector = \"#genSearchRes\\\\:id3f3bd34c5d6b1c79\\\\:id3f3bd34c5d6b1c79Navi2NumRowsInput\"\n",
        "num_courses = \"300\"\n",
        "search_field = \"#genericSearchMask\\\\:search_e4ff321960e251186ac57567bec9f4ce\\\\:cm_exa_eventprocess_basic_data\\\\:fieldset\\\\:inputField_0_1ad08e26bde39c9e4f1833e56dcce9b5\\\\:id1ad08e26bde39c9e4f1833e56dcce9b5\"\n",
        "\n",
        "select_semester_and_set_courses(driver, base_url, num_sem_selector, num_courses,\n",
        "                                sem_dropdown, search_field, num_courses_selector)"
      ],
      "metadata": {
        "id": "LnreUjDCHJu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "import datetime\n",
        "\n",
        "def screenshot_fullpage(driver, filename=None):\n",
        "    # Default-Dateiname mit Zeitstempel\n",
        "    if filename is None:\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "        filename = f\"fullpage_screenshot_{timestamp}.png\"\n",
        "\n",
        "    print(\"→ Berechne Seitenhöhe und -breite...\")\n",
        "    total_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    total_width = driver.execute_script(\"return document.body.scrollWidth\")\n",
        "\n",
        "    print(f\"→ Setze Fenstergröße: {total_width}x{total_height}\")\n",
        "    driver.set_window_size(total_width, total_height)\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    print(f\"→ Speichere Screenshot unter: {filename}\")\n",
        "    driver.save_screenshot(filename)\n",
        "\n",
        "    print(\"→ Zeige Screenshot an:\")\n",
        "    return Image(filename)"
      ],
      "metadata": {
        "id": "EqpAhX2iKsyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "screenshot_fullpage(driver)\n"
      ],
      "metadata": {
        "id": "cUvWuvI3LJ68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install html5lib\n",
        "\n",
        "#!pip install lxml html5lib"
      ],
      "metadata": {
        "id": "EJzt6iOvfenj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from tqdm.notebook import tqdm  # für Google Colab\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "def click_next_page(driver, css_next_page=\"#genSearchRes\\\\:id3f3bd34c5d6b1c79\\\\:id3f3bd34c5d6b1c79Navi2next\", max_attempts=10):\n",
        "    for attempt in range(1, max_attempts + 1):\n",
        "\n",
        "\n",
        "        # Seite nach unten scrollen\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(1)\n",
        "\n",
        "        try:\n",
        "            weiter_button = driver.find_element(By.CSS_SELECTOR, css_next_page)\n",
        "            weiter_button.click()\n",
        "            time.sleep(3)\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "        # Prüfe, ob wieder auf der Übersichtsseite (nach erfolgreichem Klick)\n",
        "        try:\n",
        "            driver.find_element(By.CSS_SELECTOR, \"#genSearchRes\\\\:genericSearchResult > div.text_white_searchresult > span\")\n",
        "            break\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "\n",
        "def clean_prefixes(df):\n",
        "    for col in df.columns:\n",
        "        prefix = str(col).strip()\n",
        "        df[col] = df[col].astype(str).str.replace(f\"^{re.escape(prefix)}\", \"\", regex=True).str.strip()\n",
        "        df[col] = df[col].replace(\"nan\", pd.NA)\n",
        "    return df\n",
        "\n",
        "\n",
        "def scrape_all_pages(driver, css_max_selector, max_pages=None):\n",
        "    # Seitenanzahl ermitteln\n",
        "    selector = driver.find_element(By.CSS_SELECTOR, css_max_selector)\n",
        "    selector_text = selector.text.strip()\n",
        "    match = re.search(r\"\\d+$\", selector_text)\n",
        "    if not match:\n",
        "        raise ValueError(\"Konnte Seitenzahl nicht extrahieren.\")\n",
        "    select_end = int(match.group())\n",
        "\n",
        "    if max_pages is not None:\n",
        "        select_end = min(select_end, max_pages)\n",
        "\n",
        "    all_tables = []\n",
        "\n",
        "    for i in tqdm(range(1, select_end + 1), desc=\"Scraping Seiten\", unit=\"Seite\"):\n",
        "        print(f\"\\n→ Starte Scraping der Base-Informationen für Seite {i}\")\n",
        "\n",
        "        table_element = driver.find_element(By.CSS_SELECTOR, \"#genSearchRes\\\\:id3f3bd34c5d6b1c79\\\\:id3f3bd34c5d6b1c79Table\")\n",
        "        table_html = table_element.get_attribute(\"outerHTML\")\n",
        "\n",
        "        soup = BeautifulSoup(table_html, \"html.parser\")\n",
        "        table = pd.read_html(StringIO(str(soup)), header=0, flavor=\"bs4\")[0]\n",
        "        table = clean_prefixes(table)\n",
        "        all_tables.append(table)\n",
        "\n",
        "        click_next_page(driver)\n",
        "        time.sleep(3)\n",
        "\n",
        "    return pd.concat(all_tables, ignore_index=True)"
      ],
      "metadata": {
        "id": "0qIXSp_eLcOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "css_max_selector = \"#genSearchRes\\\\:id3f3bd34c5d6b1c79\\\\:id3f3bd34c5d6b1c79Navi2_div > div > span.dataScrollerPageText\"\n",
        "base_info = scrape_all_pages(driver, css_max_selector, max_pages=None)"
      ],
      "metadata": {
        "id": "pAiYCjRcLkgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_info"
      ],
      "metadata": {
        "id": "mG_9eLBZhjAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "base_info.to_csv('base_info.csv', index=False)"
      ],
      "metadata": {
        "id": "50vcSTYhifEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!pip install pyjanitor\n",
        "\n",
        "import pandas as pd\n",
        "import janitor\n",
        "import re\n",
        "\n",
        "\n",
        "# Spaltennamen bereinigen und umbenennen\n",
        "base_info = base_info.clean_names().rename(columns={\"titel_der_veranstaltung\": \"titel\"})\n",
        "\n",
        "# Zeichenbereinigung im Titel\n",
        "base_info[\"titel\"] = (\n",
        "    base_info[\"titel\"]\n",
        "    .str.replace(r\"[()|+\\-!\\\".=]\", \" \", regex=True)\n",
        "    .str.replace(\"İ\", \"\", regex=False)\n",
        "    .str.slice(0, 250)\n",
        ")"
      ],
      "metadata": {
        "id": "l49d_03XrYdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_info"
      ],
      "metadata": {
        "id": "ohZLMq59vZn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from selenium.webdriver.common.by import By\n",
        "from tqdm import tqdm\n",
        "\n",
        "def scrape_missing_data(driver, missing_data, num_sem_selector, file_name):\n",
        "    total = len(missing_data)\n",
        "    result_df = pd.DataFrame()\n",
        "\n",
        "    base_name = file_name.replace(\".csv\", \"\")\n",
        "    counter = 1\n",
        "    while True:\n",
        "        new_file_name = f\"{base_name}_{counter}.csv\"\n",
        "        if not os.path.exists(new_file_name):\n",
        "            file_name = new_file_name\n",
        "            break\n",
        "        counter += 1\n",
        "\n",
        "    for i in tqdm(range(total), desc=\"Scraping Fortschritt\"):\n",
        "        titel = missing_data.iloc[i]['titel']\n",
        "        nummer = missing_data.iloc[i]['nummer']\n",
        "        print(f\"Starte das Scraping für Titel: '{titel}' (Nummer: {nummer})\")\n",
        "\n",
        "        try:\n",
        "            driver.find_element(By.CSS_SELECTOR, \"#genericSearchMask\\\\:search_e4ff321960e251186ac57567bec9f4ce\\\\:cm_exa_eventprocess_basic_data\\\\:fieldset\\\\:inputField_3_abb156a1126282e4cf40d48283b4e76d\\\\:idabb156a1126282e4cf40d48283b4e76d\\\\:termSelect_label\").click()\n",
        "            time.sleep(2.5)\n",
        "\n",
        "            css_sem_num = f\"#genericSearchMask\\\\:search_e4ff321960e251186ac57567bec9f4ce\\\\:cm_exa_eventprocess_basic_data\\\\:fieldset\\\\:inputField_3_abb156a1126282e4cf40d48283b4e76d\\\\:idabb156a1126282e4cf40d48283b4e76d\\\\:termSelect_{num_sem_selector}\"\n",
        "            driver.find_element(By.CSS_SELECTOR, css_sem_num).click()\n",
        "            time.sleep(2.5)\n",
        "\n",
        "            driver.find_element(By.CSS_SELECTOR, \"#genericSearchMask\\\\:buttonsBottom\\\\:toggleSearchShowAllCriteria\").click()\n",
        "            time.sleep(2.5)\n",
        "\n",
        "            feld_titel = driver.find_element(By.CSS_SELECTOR, \"#genericSearchMask\\\\:search_e4ff321960e251186ac57567bec9f4ce\\\\:cm_exa_eventprocess_basic_data\\\\:fieldset\\\\:inputField_0_1ad08e26bde39c9e4f1833e56dcce9b5\\\\:id1ad08e26bde39c9e4f1833e56dcce9b5\")\n",
        "            feld_titel.clear()\n",
        "            feld_titel.send_keys(titel)\n",
        "\n",
        "            start_time = time.time()\n",
        "            ccs_nummer = None\n",
        "            while time.time() - start_time < 30:\n",
        "                try:\n",
        "                    ccs_nummer = driver.find_element(By.CSS_SELECTOR, \"#genericSearchMask\\\\:search_e4ff321960e251186ac57567bec9f4ce\\\\:cm_exa_eventprocess_basic_data\\\\:fieldset\\\\:inputField_2_7cc364bde72c1b1262427dc431caece3\\\\:id7cc364bde72c1b1262427dc431caece3\")\n",
        "                    break\n",
        "                except:\n",
        "                    time.sleep(0.5)\n",
        "            if not ccs_nummer:\n",
        "                print(\"Timeout: CSS-Nummer-Feld nicht gefunden\")\n",
        "                continue\n",
        "            ccs_nummer.clear()\n",
        "            ccs_nummer.send_keys(nummer)\n",
        "            time.sleep(2.5)\n",
        "\n",
        "            driver.find_element(By.CSS_SELECTOR, \"#genericSearchMask\\\\:buttonsBottom\\\\:search\").click()\n",
        "\n",
        "            start_time = time.time()\n",
        "            ccs_find = None\n",
        "            while time.time() - start_time < 30:\n",
        "                try:\n",
        "                    ccs_find = driver.find_element(By.CSS_SELECTOR, \"#genSearchRes\\\\:id3f3bd34c5d6b1c79\\\\:id3f3bd34c5d6b1c79Table\\\\:0\\\\:tableRowAction\")\n",
        "                    break\n",
        "                except:\n",
        "                    time.sleep(0.5)\n",
        "            if not ccs_find:\n",
        "                print(\"Timeout: Finden-Button nicht gefunden\")\n",
        "                continue\n",
        "            ccs_find.click()\n",
        "            time.sleep(2.5)\n",
        "\n",
        "            semester = driver.find_element(By.CSS_SELECTOR, \"#detailViewData\\\\:tabContainer\\\\:term-selection-container\\\\:termPeriodDropDownList_label\").text\n",
        "            scraping_datum = pd.Timestamp.today().date()\n",
        "\n",
        "            # Platzhalter für echte Scraping-Funktionen\n",
        "            termine = {}  # z. B. scrape_termine(driver)\n",
        "            inhalte = {}  # z. B. scrape_inhalte(driver)\n",
        "            module = {}   # z. B. scrape_module(driver)\n",
        "            studiengaenge = {}  # z. B. scrape_studiengaenge(driver)\n",
        "\n",
        "            row_data = {\n",
        "                'semester': semester,\n",
        "                'scraping_datum': scraping_datum,\n",
        "                'titel': titel,\n",
        "                'nummer': nummer,\n",
        "                'termine': termine,\n",
        "                'inhalte': inhalte,\n",
        "                'module': module,\n",
        "                'studiengaenge': studiengaenge\n",
        "            }\n",
        "            result_df = pd.concat([result_df, pd.DataFrame([row_data])], ignore_index=True)\n",
        "\n",
        "            driver.find_element(By.CSS_SELECTOR, \"#statusLastLink1\").click()\n",
        "            time.sleep(2.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Fehler bei Titel '{titel}': {e}\")\n",
        "\n",
        "    result_df.to_csv(file_name, index=False)\n",
        "    print(f\"Daten wurden erfolgreich exportiert nach {file_name}.\")\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "awwG_ui7v-R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://www.xn--studilwe-s4a.uni-wuppertal.de/qisserver/pages/cm/exa/coursemanagement/basicCourseData.xhtml?_flowId=searchCourseNonStaff-flow&_flowExecutionKey=e3s1\"\n",
        "driver.get(base_url)\n",
        "\n",
        "# # Scraping starten\n",
        "scrape_missing_data(driver, base_info.head(4), num_sem_selector=\"2\", file_name=\"courses_2024_Winter_Wuppertal.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "DoAU4WsRwHTu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}